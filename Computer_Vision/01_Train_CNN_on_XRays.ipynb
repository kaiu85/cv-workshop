{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation von Röntgen-Thoraxbildern mit Convolutional Neural Nets\n",
    "\n",
    "In diesem Notebook wollen wir uns anschauen, was alles nötig ist, um ein neuronales Netzwerk auf Röntgenthoraxaufnahmen zu trainieren, die Normalbefunde, Befunde bei Patienten mit einer bakteriellen Pneumonie, und Befunde bei Patienten mit COVID-19 beinhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDFUQ0aUMv1-"
   },
   "source": [
    "Da Python/Jupyter nur die allernötigsten grundlegenden Funktionen und Datentypen bereitstellt, und man das Rad nicht immer neu erfinden möchte, arbeitet man *sehr viel* mit sogenannten Bibliotheken, die verschiedene vorgefertigte Funktionalitäten bereitstellen.\n",
    "\n",
    "Einige haben sie im Rahmen des Programmierkurses schon kennen gelernt. Man fügt Bibliotheken oder Teile davon mit dem \"import\" Befehl hinzu.\n",
    "\n",
    "### Cave\n",
    "\n",
    "Die Anzahl an verfügbaren Röntgen-Thoraxaufnahmen hat sich seit der Erstellung dieser Notebooks deutlich erhöht. Die Diskussion bezieht sich noch auf den Stand im Sommer 2020, als noch *sehr wenige* Röntgenthoraxaufnahmen von COVID-19 Pneumonien frei verfügbar waren. Wundern sie sich also nicht, falls die Zusammensetzung ihres Datensatzes nicht den in der Diskussion genannten Zahlenwerten entspricht.\n",
    "\n",
    "### Einrichten der Umgebung\n",
    "\n",
    "Fangen wir damit an, die Bibliotheken, die wir brauchen werden, zu importieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQY6YneXMv1-",
    "outputId": "ba40186a-f8f4-49c0-e29a-0fc95edfe759"
   },
   "outputs": [],
   "source": [
    "# PyTorch ist neben TensorFlow eines der zwei großen \"Frameworks\" für\n",
    "# Deep-Learning, es stellt die Grundlegenden Funktionen bereit, um tiefe neuronale Netze\n",
    "# zu Trainieren und Anzuwenden. Man importiert PyTorch \n",
    "# und verschiedene Sub-Module wie folgt:\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Etwas Zufall ist nie schlecht, sonst wird das Leben so langweilig\n",
    "# Random stellt Zufallszahlen und zufällige Operationen bereit, z.B. um Listen zufällig zu \"Mischen\"\n",
    "import random\n",
    "\n",
    "# Zufallszahlen können auf einem deterministischem Computer nie wirklich zufällig sein, aber es gibt\n",
    "# Algorithmen, die so schwer vorherzusagen sind, dass die resultierenden Ausgaben sehr zufällig \n",
    "# \"aussehen\". Dennoch sind diese Sequenzen immer deterministisch. D.h. startet man bei einer bestimmten Zahl,\n",
    "# so erhält man immer eine bestimmte nächste Zahl, usw.\n",
    "# Als \"Seed\" eines \"Zufallszahlengenerators\" bezeichnet man die Zahl, bei der man diese pseudozufällige Sequenz beginnt.\n",
    "random.seed(0)\n",
    "\n",
    "# Um Ergebnisse darzustellen benutzen wir matplotlib und seaborn, das auf matplotlib aufbaut.\n",
    "# Beides sind tolle Bibliotheken um schicke Abbildungen zu machen (bestimmt auch für Doktor- und andere Abschlussarbeiten nützlich)\n",
    "# matplotlib: https://matplotlib.org/\n",
    "# seaborn: https://seaborn.pydata.org/\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Numpy ist *die* Bibliothek in Python für alles, was mit Zahlen\n",
    "# und Matrizen zu tun hat.\n",
    "import numpy as np\n",
    "\n",
    "# Pandas erlaubt es uns, relativ einfach mit Daten im \"Tabellenformat\"\n",
    "# zu arbeiten\n",
    "import pandas as pd\n",
    "\n",
    "# Ein wichtiger Faktor in der Entwicklung des maschinellen Lernens war der Zuwachs an günstiger\n",
    "# paralleler Rechenkapazität, die vor allem durch die Entwicklung immer leistungsfähigerer\n",
    "# Endandwender-Graphikkarten zum Gaming vorangetrieben wurde.\n",
    "\n",
    "# Wir testen im folgenden, ob und wieviele Graphikkarten auf dem Notebook-Server zur Verfügung\n",
    "# stehen und wählen eine davon aus (bitte nicht ändern, sonst kommen sie eventuell\n",
    "# ihren Mitstudierenden ins Gehege).\n",
    "\n",
    "gpu_nr = 0\n",
    "if torch.cuda.is_available():\n",
    "    print('%d GPU(s) available.' % torch.cuda.device_count())\n",
    "    \n",
    "    if torch.cuda.device_count() > gpu_nr:\n",
    "        device = \"cuda:%d\" % gpu_nr \n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print('Using device: ' + device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyDV3uVxMv1-"
   },
   "source": [
    "### Sichten der Trainingsdaten\n",
    "\n",
    "Als nächstes bereiten wir unsere Trainingsdaten vor. Wir haben einen Datensatz aus 15537 PA-Thoraxröntgenbildern zusammengestellt, die unauffällige Befunde, Befunde bei Patienten mit einer bakteriellen Pneumonie und Befunde bei Patienten mit einer COVID-19-Pneumonie beinhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In dem Ordner 'data_path' liegen unsortiert die 15537 Bilder als jpg oder png-Dateien. \n",
    "#\n",
    "# Auf Linux und macOS Systemen werden Ordnerhierarchien mit '/' getrennt\n",
    "# '..' heißt, einen Ordner in der Hierarchie nach oben gehen. \n",
    "# '.' bezeichnet den Ordner, in dem man sich gerade befindet.\n",
    "#\n",
    "# annotation_file verweist auf eine *lange* Textdatei, die für jedes Bild \n",
    "# den Dateinamen und die Klasse (COVID, normal, pneumonia) enthält.\n",
    "# Diese heißt \"annotation.txt\" und liegt im selben Ordner wie dieses Notebook ('.').\n",
    "# Schauen sie gerne mal rein.\n",
    "\n",
    "data_path = './covid_dataset'\n",
    "annotation_file = './covid_dataset.txt'\n",
    "\n",
    "# Zunächst Lesen wir die Textdatei ein. Wir benutzen dazu das Paket \"pandas\", das es uns relativ einfach erlaubt,\n",
    "# mit Daten, die ein \"Tabellenformat\" haben, zu arbeiten.\n",
    "data_table = pd.read_csv(annotation_file, sep=' ', header=None, names=['Image','Class'])\n",
    "# Wir geben dabei der ersten Spalte den Namen \"Image\" und der zweiten Spalte den Namen \"Class\"\n",
    "\n",
    "# Der \"print\" Befehl in Python ist erstaunlich vielfältig. Meistens erhält man hilfreiche Informationen über\n",
    "# die Variable, die man ausgibt, auch wenn es sich um komplexe Strukturen handelt. Probieren wir es mal mit der\n",
    "# Tabelle aus, die wir gerade eingelesen haben.\n",
    "\n",
    "print(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie sie sehen, werden die ersten und die letzten Zeilen ausgegeben. \n",
    "# Geben wir nun nur die Zeilen aus, die COVID-19 Bilder enthalten\n",
    "\n",
    "covid = data_table[ data_table['Class'] == 'COVID-19' ]\n",
    "# Dieser etwas sperrige Ausdruckt heißt:\n",
    "# \"Gibt mir die Elemente von data_table, für die \"Class\" gleich \"COVID-19\" ist.\"\n",
    "\n",
    "print(covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P350TxHBMv1-",
    "outputId": "68a58a4e-5132-42bf-faa8-8e3e98d699dc"
   },
   "outputs": [],
   "source": [
    "# Wir erstellen nun genau so noch zwei Tabellen, die jeweils die unauffälligen Befunde (\"normal\") und\n",
    "# die der bakteriellen Pneumonien (\"pneumonia\") enthalten.\n",
    "\n",
    "normal = data_table[ data_table['Class'] == 'normal' ]\n",
    "pneumonia = data_table[ data_table['Class'] == 'pneumonia' ]\n",
    "\n",
    "# Nun zählen wir, wie viel von jeder Bildsorte wir haben. \n",
    "# len(x) gibt hierbei die Länge einer Liste an.\n",
    "# covid.index enthält die Zeilenindices (1,2,3,...) der covid-Tabelle (ganz linke Spalte)\n",
    "\n",
    "print('Number of COVID Images:')\n",
    "print(len(covid.index))\n",
    "\n",
    "print('Number of Normal Images:')\n",
    "print(len(normal.index))\n",
    "\n",
    "print('Number of Pneumonia Images:')\n",
    "print(len(pneumonia.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P350TxHBMv1-",
    "outputId": "68a58a4e-5132-42bf-faa8-8e3e98d699dc"
   },
   "outputs": [],
   "source": [
    "# Mit \"[]\" können wir auf bestimmte Spalten der Tabellen zugreifen. Die \"Image\" Spalte enthält die Dateinamen.\n",
    "# Der Einfachheit halber merken wir uns diese als Listen.\n",
    "filenames_covid = covid['Image'].tolist()\n",
    "filenames_normal = normal['Image'].tolist()\n",
    "filenames_pneumonia = pneumonia['Image'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "l7Xs67OyMv1_",
    "outputId": "60e249a6-4213-40a1-8627-6a71a3f47699"
   },
   "source": [
    "Zunächst schauen wir uns ein paar Beispiele der Trainingsdaten an.\n",
    "Dafür benutzen wir eine Hilfsfunktion, die den Pfad zu dem Bilderordner sowie eine Liste mit Dateinamen erhält und dann 5 Beispielbilder anzeigt.\n",
    "\n",
    "Wir beginnen mit PA-Aufnahmen von Patientinnen mit COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "l7Xs67OyMv1_",
    "outputId": "60e249a6-4213-40a1-8627-6a71a3f47699"
   },
   "outputs": [],
   "source": [
    "from helper_functions_covid import show_examples\n",
    "\n",
    "show_examples(data_path, filenames_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen sie die vorhergehende Zelle gerne noch einige Male aus und überzeugen sie sich, dass immer andere, zufällig gezogene Beispiele gezeigt werden.\n",
    "\n",
    "Betrachten sie die Bilder und überlegen sie sich schon einmal, welche Probleme unser Netzwerk später lösen muss.\n",
    "\n",
    "### Erste Aufgabe\n",
    "\n",
    "Als erste __kleine Aufgabe__: Schreiben sie Code, um sich auch ein paar Beispiele der Normalbefunde und der Befunde bei Patienten mit einer bakteriellen Pneumonie anzuschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "TtOD358UMv1_",
    "outputId": "2be914eb-8280-40fc-90f9-61ec37908f5a"
   },
   "outputs": [],
   "source": [
    "# Schauen wir uns noch ein paar Normalbefunde an\n",
    "\n",
    "# Hier beginnt ihr Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "TtOD358UMv1_",
    "outputId": "2be914eb-8280-40fc-90f9-61ec37908f5a"
   },
   "outputs": [],
   "source": [
    "# Und ein paar Befunde von Patienten mit einer bakteriellen Pneumonie\n",
    "\n",
    "# Hier beginnt ihr Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Klicken sie <b>hier</b> für eine mögliche Lösung</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "    \n",
    "# Normalbefunde\n",
    "show_examples(data_path, filenames_normal)\n",
    "# Bakterielle Pneumonie\n",
    "show_examples(data_path, filenames_pneumonia)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UATCdvWTZ8XS"
   },
   "source": [
    "### Endlich: Neuronale Netze\n",
    "\n",
    "Wie bereits in der Vorlesung angeklungen ist, ist es eine große Kunst für komplexe Probleme eine passende \"Architektur\" für ein künstliches neuronales Netz zu finden. D.h. die Anzahl, Typ und Reihenfolge der einzelnen Layer, sowie deren Größe, die Art der Nichtlinearitäten (\"Squashing Functions\"), ...\n",
    "\n",
    "Eine sehr gute Einführung und Wiederholung ins Thema \"Deep Learning\" finden sie hier: https://www.youtube.com/watch?v=5tvmMX8r_OM\n",
    "\n",
    "Glücklicher Weise ist eines der ersten Probleme, das gut mit künstlichen neuronalen Netzen gelöst wurde, das Erkennen von Gegenständen in Bildern. \n",
    "\n",
    "Deshalb bietet PyTorch (ebenso wie TensorFlow) schon eine relativ große Bandbreite an optimierten und gut evaluierten Architekturen an, aus denen man sich ein Netzwerk heraussuchen kann. Prinzipiell ist das, wenn man wenig bzw. keine Erfahrung mit dem Design neuronaler Netze hat, meist ein guter erster Schritt.\n",
    "\n",
    "Bei Bildern ist eine besonders beliebte und erfolgreiche Architektur die der \"Convolutional Neural Nets\". Diese Netzwerke haben eine Architektur, die gleichzeitig die Anzahl der zu optimierenden Verbindungen bzw. Gewichte reduziert und dabei dem Netz auch das implizite Vorwissen mitgibt, dass die Identität eines abgebildeten Objektes nicht von dessen Position auf dem Bild (\"Translationsinvarianz\") oder dessen Größe im Bildausschnitt (\"Skaleninvarianz\") abhängen sollte. Eine sehr gute Einführung in Bildverarbeitung mit tiefen neuronalen Netzen und in Convolutional Neural Netz finden sie hier: https://www.youtube.com/watch?v=AjtX1N_VT9E\n",
    "\n",
    "Man kann die Netze untrainiert herunterladen, d.h. alle Netzwerkgewichte werden mit zufälligen Werten initialisiert. Oder aber man kann Netzwerke herunterladen, die schon auf bestimmten Standarddatensätzen trainiert wurden. Bei Bildern ist das meist ImageNet, ein Datensatz der mehrere zehntausend Fotos von tausend verschiedenen Klassen enthält. Eine Textdatei, die die einzelnen Klassen aufzählt, finden sie in dem Ordner, in dem auch dieses Notebook liegt. Sie heißt \"imagenet_classes.txt\". Ausführliche Information zum ImageNet Datensatz und Beispielbilder einzelner Klassen finden sie auf der [ImageNet](http://image-net.org/challenges/LSVRC/2012/analysis/) Homepage.\n",
    "\n",
    "Um ein Gefühl dafür zu bekommen, wie man mit neuronalen Netzen in Python umgeht, laden wir zunächst ein auf ImageNet vortrainiertes VGG16-Netz (dabei handelt es sich um eine bestimmte Form eines Convolutional Neural Nets, s. https://arxiv.org/abs/1409.1556) herunter und zeigen diesem einige Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Wir importieren die Klasse \"vgg16\", die Methoden bereitstellt, um \n",
    "# (vortrainierte) Netze dieser Architektur zu erzeugen\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# Eine Funktion, die die Bilder so zuschneidet, dass sie zum Input Format (224x224 Pixel)\n",
    "# des Netzwerkes passen.\n",
    "from helper_functions_covid import val_transformer\n",
    "\n",
    "# Wir erzeugen eine Instanz der Modellklasse vgg16, die auf Imagenet vortrainiert wurde (pretrained == True)\n",
    "# und schicken sie mit .to(device) direkt auf die Graphikkarte, die wir für die Berechnungen benutzen\n",
    "vgg = vgg16(pretrained=True).to(device)\n",
    "\n",
    "# Zu demonstrationszwecken laden wir das (rechtefreie) Beispielbild rochen.jpg, das sich im gleichen Ordner befindet,\n",
    "# wie dieses Notebook (Tip: Über den Upload-Button der Ordner-Ansicht kann man auch eigene Dateien auf den\n",
    "# Jupyter-Server hochladen, allerdings kommt in der nächsten Zelle auch ein einfacherer Weg, wie sie das Netz\n",
    "# auf eigenen Bildern ausprobieren können)\n",
    "\n",
    "# Bild laden\n",
    "img = Image.open('rochen.jpg')\n",
    "\n",
    "# Bildgröße vorher ausgeben\n",
    "print('Bildgröße vorher:')\n",
    "print(img.size)\n",
    "\n",
    "# Bild für das Netzwerk transformieren\n",
    "img_t = val_transformer(img)\n",
    "\n",
    "# Bildgröße nachher ausgeben\n",
    "print('Bildgröße nachher:')\n",
    "print(img_t.shape)\n",
    "\n",
    "# Bild anzeigen\n",
    "plt.imshow(img_t.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Warum ist die Größe 3x224x224? Klicken sie <b>hier</b> für die Antwort.</summary>\n",
    "<p>\n",
    "Es handelt es sich um ein Farbbild mit 3 Kanälen R G B. Können Sie sich die einzelnen Kanäle anzeigen lassen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir verpacken das Bild noch so, dass das Netzwerk es als Input annimmt \n",
    "# Dazu machen wir aus dem einzelnen Bild einen Stapel (engl. \"batch\").\n",
    "# Das Netzwerk erwartet nämlich immer einen ganzen Stapel an Bildern, die parallel verarbeitet\n",
    "# werden, um die Parallelität moderner Hardware (Graphikkarten, ...) besser auszunutzen.\n",
    "# Dieser Stapel enthält in unserem Fall zwar nur ein einziges Bild, aber damit\n",
    "# das Netzwerk versteht, was es tun soll, müssen wir diese formale Konvention erfüllen.\n",
    "# Deshalb fügen wir einen \"Stapelindex\" als erste Dimension hinzu (mit unsqueeze) und schicken dann den \"Stapel\"\n",
    "# zur Graphikkarte\n",
    "batch_t = torch.unsqueeze(img_t, 0).to(device)\n",
    "\n",
    "# Wir setzen das Netzwerk in den Auswerte-Modus (manche Netze werden während des Trainings etwas anders ausgewertet\n",
    "# als wenn sie fertig trainiert im Einsatz sind)\n",
    "vgg.eval()\n",
    "\n",
    "# Dieser unscheinbare Befehl schickt unseren Bilderstapel (bestehend aus einem einzelnen Bild) \n",
    "# auf der Graphikkarte durch das neuronale Netz und liefert die Vorhersagen des neuronalen\n",
    "# Netzes für die Klasse des dargestellten Objektes. Dabei kennt das Netz natürlich nur die\n",
    "# 1000 Klassen, die im ImageNet Datensatz vorhanden sind, auf dem es trainiert wurde.\n",
    "out = vgg(batch_t)\n",
    "\n",
    "# Wir schauen uns die Output-Struktur mal an.\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir sehen, dass wir ebenfalls einen Stapel erhalten haben, der einen einzelnen Vektor mit\n",
    "# 1000 Einträgen erhält. Diese können wir in die Wahrscheinlichkeit umrechnen, die das\n",
    "# neuronale Netz für jede der 1000 ImageNet Klassen vorhergesagt hat. Dazu haben wir eine\n",
    "# kleine Hilfsfunktion geschrieben\n",
    "from helper_functions_covid import print_imagenet_predictions\n",
    "\n",
    "# Gibt die 5 Klassen mit der höchsten vorhergesagten Wahrscheinlichkeit aus\n",
    "print_imagenet_predictions(out,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Netzwerk erkennt den Rochen also relativ gut. Das liegt daran, dass \"stingray\" eine der 1000 Klassen ist, die im ImageNet-Datensatz enthalten sind und mit denen das Netz trainiert wurde. Falls sie möchten, können sie sich eine Liste der 1000 Klassen [hier](imagenet_classes.txt) anschauen.\n",
    "\n",
    "Beispiele der Bilder und eine genaue Beschreibung des Datensatzes, mit dem die Netze vortrainiert worden sind, finden sie auf der [ImageNet](http://image-net.org/challenges/LSVRC/2012/analysis/) Homepage.\n",
    "\n",
    "Wenn sie die nächste Zelle ausführen, erscheint ein kleiner Upload-Button. Falls sie möchten, können sie hier ein eigenes Bild (z.B. aus dem Web) hochladen und das Netzwerk einen Tipp abgeben lassen, was darauf zu sehen sein könnte.\n",
    "\n",
    "__Beachte:__ Nach dem erfolgreichen Upload ändert sich die Zahl hinter \"Upload\" von \"(0)\" zu \"(1)\".\n",
    "Führen sie dann die nächste Zelle auf, um das Bild anzuzeigen und die Vorhersage zu starten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Die nächsten 3 Zeilen erzeugen nur ein Upload Widget\n",
    "import ipywidgets as widgets\n",
    "uploader = widgets.FileUpload()\n",
    "uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions_covid import predict_imagenet_class_from_upload\n",
    "\n",
    "# Formatiert den Upload als Stapel, schickt ihn durchs Netzwerk, wertet die\n",
    "# Vorhersagewahrscheinlichkeiten aus und zeigt die Top-5 Klassen an. \n",
    "# Ganz analog zum Beispiel oben.\n",
    "predict_imagenet_class_from_upload(vgg, uploader, 5, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgefertigte (und evtl. auch vortrainierte) Netzwerke an eigene Daten anpassen\n",
    "\n",
    "Das Netz, das wir gerade geladen haben wurde trainiert um auf Fotos 1000 (tausend!) verschiedene Objektklassen zu erkennen. Das gleiche gilt für viele weitere CNN-Architekturen, die auf dem ImageNet-Datensatz entwickelt und trainiert wurden.\n",
    "\n",
    "Wir können dieses Netz bereits trainiert herunterladen, in der Hoffnung, dass gewisse Features, z.B. Ecken, Kanten oder Texturen, die bei der Klassifikation von Autos, Hunden, Topfpflanzen oder anderen Gegenständen eine Rolle spielen, uns auch bei unserem Problem weiterhelfen werden. Wir können die Netzwerke jedoch auch zufällig initialisiert laden und lediglich die vorgefertigte Netzwerkarchitektur nutzen.\n",
    "\n",
    "So oder so: Da wir in unseren Daten jedoch 3 Klassen (normal, bakterielle Pneumonie, COVID-Pneumonie) haben, die bereitgestellten Netzwerkarchitekturen jedoch auf dem ImageNet-Datensatz mit 1000 Klassen trainiert und evaluiert worden sind, müssen wir solche vorgefertigen Netzwerk etwas verändern, damit sie auf unsere Daten anwendbar sind. \n",
    "\n",
    "In der Praxis ist es meist so, dass das letzte Layer eines Klassifikationsnetzwerkes ein \"Fully-Connected\" Layer ist, welches eine hochdimensionalen Repräsentation des präsentierten Bildes auf ein Output-Layer weiterleitet, in dem jedes Output-Neuron für eine der Objektklassen steht.\n",
    "\n",
    "In dem VGG-Netzwerk oben beinhaltet das letzte \"hidden layer\" z.B. 4096 Neurone, die eine gelernte, kompakte Repräsentation des gezeigten Bildes darstellen. Diese werden über ein Fully-Connected Layer mit den 1000 Output-Neuronen verbunden, eines für jede ImageNet-Klasse.\n",
    "\n",
    "In solchen Fällen müssen wir lediglich die Anzahl der Output-Neurone auf die Anzahl der Klassen in unserem Datensatz ändern und das letzte Fully-Connected Layer entsprechend anpassen.\n",
    "\n",
    "Wir haben für sie eine Hilfsfunktion bereitgestellt, die dies für fünf bekannte Netzwerkarchitekturen erledigt. Diese sind \"VGG\" (s. oben), \"AlexNet\", \"DenseNet\", \"ResNet\" und \"SqueezeNet\". Sie können zudem auswählen, ob das Netz mit vortrainierten Gewichten (auf dem ImageNet-Datensatz) oder mit zufällig initialisierten Gewichten geladen werden soll. Ob das Vortrainieren auf natürlichen Bildern tatsächlich Vorteile für neuronale Netze auf __medizinischen Bildgebungsdaten__ liefert, ist noch Gegenstand aktueller Forschung. Die finale Performance der Netze, wenn diese lange auf großen medizinischen Datensätzen trainiert werden, wird durch Vortraining auf natürlichen Bildern meist nicht signifikant verbessert. Die Trainingsgeschwindigkeit und die Ergebnisse auf kleinen Datensetzen können jedoch eventuell positiv beeinflusst werden.\n",
    "\n",
    "Auf dieser [Seite](https://www.jeremyjordan.me/convnet-architectures/) finden sie kurze Steckbriefe von VGG-16, AlexNet, DenseNet, und ResNet. Diese Netzwerke haben mehrere zehn Millionen freie Parameter oder mehr. Außerdem haben wir eine SqueezeNet-Architektur vorbereitet. Dieses heißt so, weil sie entwickelt wurde, um auf ein mobiles Endgerät \"gequetscht\" zu werden, so dass z.B. ein Smartphone auch Objekte erkennen kann, ohne dass vorher Bilder \"in die Cloud\" (d.h. auf den Computer von jemand anderem) geschickt werden müssen. Einen kurzen Überblick über SqueezeNet finden sie z.b. [hier](https://towardsdatascience.com/review-squeezenet-image-classification-e7414825581a).\n",
    "\n",
    "Im folgenden erzeugen wir unser angepasstes Netzwerk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e79517da3159458c8a91cea11d5bfeea",
      "82060550eeac4dae9eb34252d2e8f641",
      "f86e1989a14e4735b76f83058f3781b1",
      "27f1d0243add4eb684ea2968d6c71b33",
      "2f516019eb554f4893d2241601925ee5",
      "9563661e32ed40a585f1f939b957e28e",
      "076aaecb0dde496faa19e1de042e3065",
      "1fc11a8d190f4983a5f4f818fc904f61"
     ]
    },
    "id": "PpWHkHb5Mv2A",
    "outputId": "490b88e7-8297-4ba3-f731-1820f0496481"
   },
   "outputs": [],
   "source": [
    "from helper_functions_covid import get_pretrained_model\n",
    "# Für die Modellarchitektur können sie aus folgenden Architekturen wählen:\n",
    "# 'vgg', 'squeezenet', 'resnet', 'alexnet' und 'densenet' \n",
    "#\n",
    "# Mit dem pretrained-Parameter bestimmen sie, ob das Netz vortrainiert (True) \n",
    "# oder zufällig initialisiert (False) geladen werden soll.\n",
    "model = get_pretrained_model('squeezenet', pretrained = True)\n",
    "\n",
    "# Wir schicken das Netzwerk auf die Graphikkarte\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Output gibt uns eine kurze Zusammenfassung der Netzwerkarchitektur. Dieser ist leider relativ schwer zu lesen. Falls sie sich für die Details interessieren, gibt es mehr Details [hier](https://towardsdatascience.com/using-predefined-and-pretrained-cnns-in-pytorch-e3447cbe9e3c).\n",
    "\n",
    "In der nächsten Zelle können wir eine Funktion benutzen, die uns eine etwas strukturiertere Übersicht über die Architektur gibt, und auch eine Abschätzung, wie viel Arbeitsspeicher auf der Graphikkarte das Training mit einer gegebenen Batch-Size (\"Stapelgröße\", Anzahl der Trainingsbeispiele, die parallel verarbeitet werden) benötigt.\n",
    "\n",
    "Nur für das Densenet funktioniert das leider nicht, da die Bibliothek die wir nutzen, nicht mit der DenseNet-Architektur zurecht kommt, in der jedes Layer eines \"Dense\"-Blockes nicht nur Inputs von direkt vorhergehenden Layers, sondern von *allen* vorhergehenden Layers des Blockes bekommt. So ist es leider manchmal.\n",
    "\n",
    "Probieren sie gerne die verschiedenen Netzwerkarchitekturen aus und vergleichen sie z.B. die Anzahl der Parameter und den Speicherverbrauch von __VGG und SqueezeNet__.\n",
    "\n",
    "Schauen sie auch gerne, was mit dem Arbeitsspeicherverbrauch passiert, wenn sie den __batch_size__-Parameter ändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e79517da3159458c8a91cea11d5bfeea",
      "82060550eeac4dae9eb34252d2e8f641",
      "f86e1989a14e4735b76f83058f3781b1",
      "27f1d0243add4eb684ea2968d6c71b33",
      "2f516019eb554f4893d2241601925ee5",
      "9563661e32ed40a585f1f939b957e28e",
      "076aaecb0dde496faa19e1de042e3065",
      "1fc11a8d190f4983a5f4f818fc904f61"
     ]
    },
    "id": "PpWHkHb5Mv2A",
    "outputId": "490b88e7-8297-4ba3-f731-1820f0496481",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary \n",
    "\n",
    "# Mit dem Summary Befehl erhalten wir eine kurze Zusammenfassung\n",
    "# der Architektur unseres Modells, und wieviel Speicher es verbrauchen\n",
    "# würde, wenn man acht Bilder mit drei Farbkanälen und 224x224 Pixeln\n",
    "# verarbeiten würde. Damit bekommt man auch eine realistische Einschätzung,\n",
    "# wieviel Arbeitsspeicher man dafür auf der Graphikkarte braucht.\n",
    "if model.name != 'densenet':\n",
    "    summary(model,input_size = (3,224,224),batch_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate und Optimizer\n",
    "\n",
    "Wie in der Vorlesung besprochen, ist bei gradientenbasierten Optimierungsverfahren die Wahl einer passenden \"Schrittweite\" in Richtung der Gradienten der Zielfunktion wichtig. Ist die Schrittweite zu klein, dauert es mitunter sehr, sehr lange, bis das Netzwerk eine gute Performance erreicht. Ist die Schrittweite zu groß, kann es tatsächlich sein, dass es überhaupt nichts sinnvolles lernt, da es immer wieder aus den entsprechenden lokalen Minima der Zielfunktion herausspringt.\n",
    "\n",
    "Als kleinen Exkurs können sie in der nächsten Zelle mit einer Funktion spielen, die einen Gradientenabstieg auf einer eindimensionalen, quadratischen Zielfunktion simuliert. D.h. es gibt nur einen einzigen, skalaren Parameter theta, und die Zielfunktion ist eine quadratische Funktion dieses Parameters. Sie können mit __theta_start__ bestimmen, von welchem Parameter-Wert der Gradientenabstieg starten soll. Zudem können sie die Lernrate __learning_rate__ bestimmen, die skaliert, wie groß die Optimierungsschritte in Richtung des Minimums sein sollen. Außerdem können sie mit __n_steps__ angeben, wie viele Optimierungsschritte gemacht werden sollen.\n",
    "\n",
    "Als Output erhalten sie zwei Graphen. Der Linke zeigt die Zielfunktion (blau) als Funktion des Parameters theta, und wie sich die Parameter-Werte mit jedem Optimierungsschritt vom Startwert (dunkelblauer Marker) zum finalen Wert (roter Marker) ändern (orangene Pfeile).\n",
    "\n",
    "Rechts sehen sie den Wert der Kostenfunktion als Funktion des Optimierungsschrittes. Ein funktionierender Optimiser sollte am Schluss ziemlich nahe am Minimum __0__ der Kostenfunktion sein.\n",
    "\n",
    "Spielen sie etwas mit den Parametern. Beobachten sie dabei, was passiert wenn sie die Lernrate sehr klein (z.B. 0.01) oder sehr groß (z.B. 2.0) machen. Schauen sie, wie schnell der Algorithmus für sinnvolle Werte (in diesem Beispiel z.B. 0.3) der Lernrate zum Minimum konvergiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from helper_functions_covid import simulate_gradient_descent_on_quadratic_potential\n",
    "\n",
    "# Lernrate (probieren sie z.B. 0.02, 0.05, 0.1, 0.2, 0.3, 0.6, 0.9, 1.0, 1.1)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Anzahl der simulierten Schritte\n",
    "n_steps = 10\n",
    "# Startwert für den Parameter theta\n",
    "theta_start = -2.0\n",
    "simulate_gradient_descent_on_quadratic_potential(theta_start, learning_rate, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sie sehen, hat die Wahl der Lernrate einen kritischen Einfluss auf das Konvergenzverhalten von gradientenbasierten Optimierungsverfahren. Umso unbefriedigender ist es, dass es noch keine wirklich fundierte Methode gibt, diese festzulegen. In der Praxis ist meistens eine Menge Ausprobieren involviert, in dem man z.B. numerische \"Experimente\" durchführt, die einen ganzen Bereich von Lernraten ausprobieren.\n",
    "\n",
    "Üblicher Weise funktionieren Lernraten im Bereich von 0.00001 - 0.001.\n",
    "\n",
    "In der folgenden Zelle haben sie die Wahl der Lernrate und des gradientenbasierten Optimierungsverfahrens für unser Convolutional Neural Net. Auch hier gibt es einen ganzen Forschungszweig, der sich damit beschäftigt, neue Optimierungsverfahren zu entwickeln, die schneller und zuverlässiger konvergieren. Wir bieten ihnen hier zwei Verfahren an:\n",
    "- SGD oder \"stochastic gradient descent\" ist der absolute Klassiker, der tatsächlich genau das macht, was wir Ihnen in der Vorlesung gesagt haben. Also: Den Gradienten der Zielfunktion bestimmen und dann einen Schritt entgegen diesem Gradienten (erinnern sie sich, dass der Gradient einer Funktion die Richtung des __steilsten Anstieges__ ist) gehen, der mit der Lernrate skaliert wird. Dieser Algorithmus konvergiert bei genügend kleinen Lernraten fast immer und findet Minima, die gut generalisieren. Er ist jedoch auch sehr __langsam__. In der erweiterten Version, die wir hier benutzen gibt es noch einen Parameter, __\"momentum\"__, der dem Optimierungsprozess eine gewisse __Trägheit__ verleiht. Stellen sie sich eine schwere Metallkugel vor, die einen bergigen Hang hinunterrollt. Wenn diese auf eine kleine Mulde trifft, wird sie nicht in diesem lokalen Minimum hängen bleiben, sondern aufgrund ihrer Trägheit darüber hinwegrollen. \n",
    "- ADAM oder \"adaptive moment estimation\" hat sich seit seiner [Publikation](https://arxiv.org/pdf/1412.6980.pdf) 2014 zum \"Schweizer Taschenmesser\" der gradientenbasierten Optimierungsverfahren entwickelt. Das liegt daran, dass dieser Algorithmus, der seine Schrittweite während des Optimierungsprozesses anpasst, mit seinen Standardparametern über viele Anwendungen, Netzwerkarchitekturen und Zielfunktionen hinweg nicht immer die beste, aber immer eine passable Leistung bietet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "QEcHDy1UMv2A",
    "outputId": "b234e40d-5e5e-428d-f4cf-0bc298a5d7e1"
   },
   "outputs": [],
   "source": [
    "# Wählen sie eine Lernrate zwischen 0.00001 und 0.001\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Wählen sie einen der beiden Optimizer, in dem sie die entsprechende Zeile einkommentieren.\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainings-, Test- und Validierungsset\n",
    "\n",
    "Ein großes Problem an Modellen, die Millionen an freien Parametern haben, ist das sogenannte Overfitting. Hierbei ist das Modell so flexibel, dass es nicht nur die Regularitäten bzw. die Struktur in den Daten aufgreift, sondern auch das __zufällige Rauschen in den Daten lernt__. Das führt natürlich dazu, dass solch ein zu flexibles Modell schlecht generalisiert, d.h. schlecht neue Datenpunkte, die ein anderes zufälliges Rauschen mitbringen, vorhersagt.\n",
    "\n",
    "In der nächsten Zelle können sie ausprobieren, einen Datensatz (blaue Punkte), der auf Grundlage einer deterministischen Funktion (orange) mit additivem, zufälligen Rauschen generiert wurde, durch ein Polynom anzupassen. Dabei gibt der Grad des Polynoms an, wie viele freie Parameter angepasst werden können. Ein Grad von eins entspricht dabei einer Geraden mit zwei freien Parametern (Steigung und y-Achsenabschnitt). Ein Grad von zwei entspricht einer quadratischen Funktion mit drei freien Parametern, usw...\n",
    "\n",
    "In der folgenden Zelle können sie der Funktion __plot_overfitting_demo__ eine Liste mit Graden (1,2,3,...) übergeben, für die jeweils ein Polynom des entsprechenden Grades an die Stichprobe gefittet wird. Die entsprechenden Graphen werden nebeneinander dargestellt (ab 4 wird es ziemlich gequetscht).\n",
    "\n",
    "Probieren sie gerne einmal aus, wie sich die Annäherung an die Daten verändert, wenn man von sehr einfachen Funktionen (Grad 1) zu __sehr, sehr komplizierten (Grad 100) geht__. Beachten sie dabei, dass mit zunehmendem Grad der mittlere quadratische Fehler auf der Stichprobe immer kleiner wird, die Annäherung der __wahren Funktion (orange)__ durch das __Modell (\"Polynom\", blau)__ aber nicht unbedingt besser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions_covid import plot_overfitting_demo\n",
    "\n",
    "# Gerne auch mehrere Grade in die Liste eintragen, zum vergleichen\n",
    "grade = [1,2] \n",
    "\n",
    "plot_overfitting_demo(grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Praxis hat sich im Deep Learning folgendes Vorgehen durchgesetzt, um Overfitting zu vermeiden:\n",
    "\n",
    "Man teilt die Daten, mit denen der Algorithmus trainiert wird in drei __unabhängige__ Teilmengen:\n",
    "\n",
    "- Ein __Trainingsset__: Auf dem Trainingsset wird der Gradient der Kostenfunktion berechnet, der von dem gradientenbasierten Optimierungsalgorithmus genutzt wird, um die Parameter der neuronalen Netze zu optimieren.\n",
    "\n",
    "- Ein __Validierungsset__: Auf dem Validierungsset wird während des gradientenbasierten Trainings immer wieder die Kostenfunktion ausgewertet. So lange das neuronale Netz noch sinnvolle Struktur aus dem Trainingsset lernt, sollte auch die Kostenfunktion auf dem Validierungsset fallen. Sobald während des Trainings jedoch der Wert der Kostenfunktion auf dem Validierungsset zu steigen beginnt, während er auf dem Trainingsset weiter fällt, geht man davon aus, dass das Netz beginnt zu \"überfitten\" und beendet das Training. Dies wird in der Literatur als __early stopping__ bezeichnet.\n",
    "\n",
    "- Ein __Testset__: Das Testset wird benutzt, um die finale Performance des trainierten Netzwerkes zu überprüfen. Es muss vom Trainings- und Validierungsset möglichst komplett unabhängig sein.\n",
    "\n",
    "Bevor wir also mit dem Training unseres Netzes auf den Röntgenbildern beginnen können, teilen wir die drei Listen, die die Röntgenbilder der Normalbefunde, COVID-Erkrankungen und bakteriellen Pneumonien beinhalten, jeweils in diese drei Untermengen auf.\n",
    "\n",
    "Dafür haben wir eine Helferfunktion geschrieben, __split_list__, die die Listen zufällig in ein Trainings-, Validierungs- und Testset aufteilt. Man kann der Funktion die relative, prozentuale Aufteilung in die drei Sets in Form der Parameter train_percentage, validation_percentage, und test_percentage übergeben.\n",
    "\n",
    "Üblich sind z.B. ein Split von 80% der gesamten Trainingsdaten als Trainingsset, und je 10% als Validierungs- und Testset.\n",
    "\n",
    "Wählen sie auch gerne eine andere Aufteilung, z.B. 60%/20%/20%.\n",
    "\n",
    "Nebenbemerkung: Eine weitere Technik, um Overfitting zu vermeiden, die wir zunächst nicht verwenden, ist das sogenannte Dropout. Mehr dazu finden sie hier: https://www.youtube.com/watch?v=5tvmMX8r_OM&t=2989s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions_covid import split_list\n",
    "\n",
    "train_percentage = 0.8\n",
    "valid_percentage = 0.1\n",
    "test_percentage = 0.1\n",
    "\n",
    "# Nun können wir die Listen entsprechend aufteilen und uns die resultierenden\n",
    "# Aufteilungen anschauen\n",
    "\n",
    "# Zunächst für die COVID-Bilder\n",
    "\n",
    "filenames_covid_train, filenames_covid_val, filenames_covid_test = \\\n",
    "    split_list(filenames_covid, train_percentage, valid_percentage, test_percentage)\n",
    "\n",
    "print('Training Cases with COVID:')\n",
    "print(len(filenames_covid_train))\n",
    "\n",
    "print('Validation Cases with COVID:')\n",
    "print(len(filenames_covid_val))\n",
    "\n",
    "print('Test Cases with COVID:')\n",
    "print(len(filenames_covid_test))\n",
    "\n",
    "# Nun für die Normalbefunde\n",
    "\n",
    "filenames_normal_train, filenames_normal_val, filenames_normal_test = \\\n",
    "    split_list(filenames_normal, train_percentage, valid_percentage, test_percentage)\n",
    "\n",
    "print('Normal Training Cases:')\n",
    "print(len(filenames_normal_train))\n",
    "\n",
    "print('Normal Validation Cases:')\n",
    "print(len(filenames_normal_val))\n",
    "\n",
    "print('Normal Test Cases:')\n",
    "print(len(filenames_normal_test))\n",
    "\n",
    "# Und nun für die Befunde bei bakteriellen Pneumonien\n",
    "\n",
    "filenames_pneumonia_train, filenames_pneumonia_val, filenames_pneumonia_test = \\\n",
    "    split_list(filenames_pneumonia, train_percentage, valid_percentage, test_percentage)\n",
    "\n",
    "print('Pneumonia Training Cases:')\n",
    "print(len(filenames_pneumonia_train))\n",
    "\n",
    "print('Pneumonia Validation Cases:')\n",
    "print(len(filenames_pneumonia_val))\n",
    "\n",
    "print('Pneumonia Test Cases:')\n",
    "print(len(filenames_pneumonia_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die __Nachvollziehbar- und Reproduzierbarkeit__ sollten solche zufälligen Aufteilungen (__\"Splits\"__) auch immer __dokumentiert__ werden. Deshalb schreiben wir die einzelnen Listen in der nachfolgenden Zelle in einzelne Textdateien, die sie sich auch gerne über das Jupyter-Interface anschauen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions_covid import write_list_to_textfile\n",
    "\n",
    "write_list_to_textfile(filenames_covid_train,'filenames_covid_train.txt')\n",
    "write_list_to_textfile(filenames_covid_test,'filenames_covid_test.txt')\n",
    "write_list_to_textfile(filenames_covid_val,'filenames_covid_val.txt')\n",
    "\n",
    "write_list_to_textfile(filenames_normal_train,'filenames_normal_train.txt')\n",
    "write_list_to_textfile(filenames_normal_test,'filenames_normal_test.txt')\n",
    "write_list_to_textfile(filenames_normal_val,'filenames_normal_val.txt')\n",
    "\n",
    "write_list_to_textfile(filenames_pneumonia_train,'filenames_pneumonia_train.txt')\n",
    "write_list_to_textfile(filenames_pneumonia_test,'filenames_pneumonia_test.txt')\n",
    "write_list_to_textfile(filenames_pneumonia_val,'filenames_pneumonia_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Testset brauchen wir erst später, wenn wir die finale Performance unseres trainierten Netzes beurteilen wollen. __Es schadet aber nie, schon früh daran zu denken, die entsprechenden Daten getrennt von Trainings- und Validierungsset zur Seite zu legen.__\n",
    "\n",
    "In der nächsten Zelle erstellen wir nun Datensätze, aus denen entsprechende Stapel (\"Batches\") zum Training des Netzes gezogen werden können.\n",
    "\n",
    "Eine weitere Möglichkeit, Overfitting zu verhindern, ist die Daten zu __augmentieren__. __Datenaugmentierung__ bezeichnet das Vorgehen, dass man auf die Trainingsdaten, bevor man sie dem Netz präsentiert, Transformationen anwendet, von denen der Output des Netzes unabhängig sein sollte. Geht es zum Beispiel um das Erkennen von Objekten auf Bilder, die an verschiedenen Orten und in verschiedenen Größen auftauchen können, kann man z.B. die Trainingsbilder zufällig verschieben und skalieren. So verhindert man, dass das Netz z.B. lernt, das Hunde immer links unten und Katzen immer rechts oben in den Trainingsbildern waren.\n",
    "\n",
    "Wir haben eine Sammlung solcher Operationen schon in der \"train_transformer\"-Struktur zusammengefasst. Diese beinhaltet zufälliges horizontales Spiegeln (quasi vertauschen der rechten und der linken Lunge), und zufälliges Skalieren (etwas vergrößern und verkleinern).\n",
    "\n",
    "Gleichzeitig werden die Grauwerte normalisiert, so dass sie einen Mittelwert von 0 und eine Standardabweichung von 1 haben. Damit wird auch eine Abhängigkeit vom absoluten Grauwert verhindert.\n",
    "\n",
    "Zudem werden die Bilder auf 224x224 Pixel skaliert, da die Architekturen, die wir benutzen, auf diese Bildgröße ausgerichtet sind, die der Größe der Bilder im ImageNet-Datensatz entspricht.\n",
    "\n",
    "Ein wichtiger freien Parameter des Trainings ist auch die Batchsize, die angibt, wie viele Bilder auf einen Trainingsstapel gepackt werden.\n",
    "\n",
    "Entsprechend des Arbeitsspeichers, der auf den Graphikkarten zur Verfügung steht, und der Größe der Netzwerke ist eine Batch-Size von 8 empfehlenswert. Wenn man ein SqueezeNet-Trainiert kann die Batch-Size allerdings auch erhöht werden. Probieren sie dann gerne 16 aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xta8wBB6Mv2A"
   },
   "outputs": [],
   "source": [
    "from helper_functions_covid import ImageDataset, train_transformer\n",
    "\n",
    "batchsize = 8\n",
    "\n",
    "trainset = ImageDataset(root_dir=data_path,\n",
    "                          classes = ['normal', 'pneumonia', 'COVID'],\n",
    "                          files_path = [filenames_normal_train, filenames_pneumonia_train, filenames_covid_train],\n",
    "                          transform= train_transformer)\n",
    "\n",
    "valset = ImageDataset(root_dir=data_path,\n",
    "                          classes = ['normal', 'pneumonia', 'COVID'],\n",
    "                          files_path = [filenames_normal_val, filenames_pneumonia_val, filenames_covid_val],\n",
    "                          transform= val_transformer)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir mit dem Training anfangen können, brauchen wir noch eine Struktur, die auf das Early-Stopping achtet, also den Trainingsvorgang beendet, wenn die Kostenfunktion auf dem Validierungsset beginnt anzusteigen. Da der Optimierungsprozess aufgrund des zufälligen Ziehens der einzelnen Trainignsstapel auch ein Zufallselement beinhaltet, kann es immer mal vorkommen, dass so ein Anstieg kurz auftritt, ohne dass das Netz bereits zu überfitten beginnt. Deshalb übergibt man EarlyStopping einen Parameter, \"patience\", der angibt, über wie viele Durchgänge durch das Trainingsset hinweg die Kostenfunktion auf dem Validierungsset kontinuierlich ansteigen darf, bis dem early_stopper \"die Geduld ausgeht\" und das Training beendet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPcNDzx2Mv2A"
   },
   "outputs": [],
   "source": [
    "from helper_functions_covid import EarlyStopping\n",
    "\n",
    "early_stopper = EarlyStopping(patience = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostenfunktion\n",
    "\n",
    "Nachdem wir nun einen Datensatz und eine Netzwerkarchitektur haben, müssen wir noch festlegen, welche Kostenfunktion wir minimieren wollen, d.h. mit welcher Kostenfunktion wir die Gradienten bestimmen, die dem Optimizer die Schrittrichtung vorgeben.\n",
    "\n",
    "Bei Klassifikationsproblemen wählt man meistens die sogenannte \"Kreuzentropie\" oder \"Cross-Entropy\". Das klingt zwar sehr kompliziert, aber im wesentlichen funktioniert diese Kostenfunktion so:\n",
    "\n",
    "- Jede Klasse erhält ein Output-Neuron\n",
    "- Für jedes Trainingsbeispiel werden die Aktivierungen der Output-Neuronen in eine Wahrscheinlichkeitsverteilung über die einzelnen Klassen umgerechnet, so dass eine höhere Aktivierung eines bestimmten Output-Neurons einer höheren Wahrscheinlichkeit der entsprechenden Klasse entspricht. Dies wird in der Praxis gemacht, in dem man die Aktivierungen der Output-Neurone durch eine sogenannte SoftMax-Funktion schickt.\n",
    "- Die Kreuzentropie für einen Satz (i.e. ein Batch oder einen Stapel) Trainingsbeispiele ist dann die __mittlere Wahrscheinlichkeit der tatsächlichen, wahren Klasse unter der von Netz vorhergesagten Wahrscheinlichkeitsverteilung__. D.h. diese Zielfunktion sorgt dafür, dass die Wahrscheinlichkeit, die das Netz der tatsächlichen Klasse eines Trainingsbeispiels gibt, immer größer wird. Oder anders gesagt: Dass die Aktivität des Output-Neurons, dass die wahre Klasse eines Trainingsbeispiels repräsentiert, größer wird.\n",
    "\n",
    "Eine ausführliche Beschreibung dieses Zusammenhanges finden sie z.B. in diesem tollen [Blogartikel](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451).\n",
    "\n",
    "Zum Glück bring PyTorch (wie auch TensorFlow) die meisten gebräuchlichen Kostenfunktionen schon mit, so dass es reicht, die entsprechende Struktur aus PyTorchs NeuralNetwork (torch.nn) Bibliothek zu benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluationsmetriken\n",
    "\n",
    "Neben der Zielfunktion, die benutzt wird, um auf dem Trainingsset den Gradienten (die Ableitung) der Zielfunktion nach den Netzwerkparametern zu berechnen, gibt es noch weitere Metriken, mit denen wir die Performance des Netzes evaluieren können.\n",
    "\n",
    "### Recall auf den einzelnen Trainingsklassen\n",
    "Der Recall ist eine Verallgemeinerung der \"Sensitivität\", die wir schon von einfachen positiv/negativ Tests kennen. Diese kann man als binäre (d.h. es gibt nur zwei Klassen) Classifier betrachten, die Personen in zwei Klassen (gesund/erkrankt) klassifizieren. Hier ist die Sensitivität definiert als die bedingte Wahrscheinlichkeit, dass der test positiv ausfällt, wenn die Person tatsächlich erkrankt ist.\n",
    "\n",
    "$$\\mathrm{Sensitivität = P(positiv|erkrankt)}$$\n",
    "\n",
    "So eine bedingte Wahrscheinlichkeit kann man für Classifier, die mehrere Klassen zuordnen, für jede einzelne Klasse definieren. Dieses Größe wird der \"Recall für die Klasse K\" genannt und ist dabei definiert als\n",
    "\n",
    "$$\\mathrm{Recall(K) = P(Classifier\\ ordnet\\ Klasse\\ K\\ zu\\ |\\ Datenpunkt\\ hat\\ tatsächlich\\ Klasse\\ K)}$$\n",
    "\n",
    "__Beachte__, dass unter dieser Definition die \"Sensitivität\" eines binären Tests der Recall für die Klasse \"erkrankt\" ist, und die \"Spezifität\" der Recall für die Klasse \"gesund\".\n",
    "\n",
    "### Accuracy\n",
    "Die Accuracy eines Classifiers auf einem Datensatz ist der relative Anteil der richtig Klassifizierten Punkte. Diese Metrik ist mit __großer__ Vorsicht zu genießen, wenn die einzelnen Klassen in den Trainingsdaten nicht gleich Verteilt sind. Dies ist auch in unserem Beispiel der Fall. Erinnern wir uns an die Verteilung der Klassen in unserem gesamten Datensatz:\n",
    "\n",
    "```python\n",
    "    \n",
    "Number of COVID Images:\n",
    "617\n",
    "Number of Normal Images:\n",
    "8851\n",
    "Number of Pneumonia Images:\n",
    "6069\n",
    "    \n",
    "```\n",
    "\n",
    "D.h. wenn wir einen (offensichtlich nutzlosen) Classifier hätten, der __allen Datenpunkten__ die Klasse __normal__ zuordnet, hätte dieser schon eine Accuracy von $8851/(8851 + 6069 + 617) \\approx 0.57 = 57\\%$. Wenn wir einen Classifier hätten, der alle Bilder der Klassen \"normal\" und \"pneumonia\" richtig klassifiziert, aber die Klasse \"covid\" gar nicht erkennt (sondern die entsprechenden Bilder einer der anderen Klassen zuordnet) wäre die Accuracy schon $(8851+6069)/(8851 + 6069 + 617) \\approx 0.96 = 96\\%$. Das zeigt, wie sehr diese Kenngröße mit Vorsicht zu genießen ist.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Ähnlich die der Recall die Sensitivität und Spezifität verallgemeinert, verallgemeinert die Precision den Positiven Prädiktiven Wert und den Negativen Prädiktiven Wert eines binären Tests. Die Precision für eine Klasse K ist definiert als die bedingte Wahrscheinlichkeit, dass ein Datenpunkt tatsächlich zu Klasse K gehört, wenn der Classifier ihm Klasse K zuordnet. \n",
    "\n",
    "$$\\mathrm{ Precision(K) = P(Datenpunkt\\ hat\\ tatsächlich\\ Klasse\\ K\\ |\\ Classifier\\ ordnet\\ Klasse\\ K\\ zu) }$$\n",
    "\n",
    "Erinnern wir uns kurz an die bekannten Ausdrücke für binäre Tests: Der Positive Prädiktive Wert ist die Wahrscheinlichkeit, dass eine Person tatsächlich erkrankt ist, wenn der Test positiv ausfällt. Der Negative Prädiktive Wert ist die Wahrscheinlichkeit, dass eine Person tatsächlich gesund ist, wenn der Test negativ Ausfällt. Demnach kann man den Positiven Prädiktiven Wert als die Precision des binären Classifiers (Klassen: gesund/erkrankt) bezüglich der Klasse \"erkrankt\" betrachten und den Negativen Prädiktiven Wert als die Precision des Classifiers bezüglich der Klasse \"gesund\".\n",
    "\n",
    "Ebenso wie bei den binären Tests gilt für die Precision bezüglich einer Klasse: Sie ist nicht nur abhängig von dem Recall des Classifiers für diese und die anderen Klassen (Verallgemeinerung von Sensitivität/Spezifität), sondern auch von der __Vortestwahrscheinlichkeit__ der einzelnen Klassen. D.h. ebenso wie die Accuracy hängt die Precision nicht nur von den eigenschaften des Classifiers, sondern auch von der Verteilung der einzelnen Klassen im jeweiligen Trainings-, Validierungs- und Testdatensatz ab. Dementsprechend nutzen wir diese Größe im Folgenden nicht, sondern beschränken uns damit, den Recall des Classifiers für die einzelnen Klassen anzugeben, der (ganz analog zu Sensitivität und Spezifität) eher die Eigenschaften des Tests bzw. Classifiers __an sich__ beschreibt.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Precision und Recall als bedingte Wahrscheinlichkeiten können aus der Confusion-Matrix eine Classifiers angegeben werden. Diese gibt für eine Menge an Datenpunkten an, wie viele der gesamten Datenpunkte welche wahre Klasse haben und welche Klasse vom Classifier zugeordnet bekommen.\n",
    "\n",
    "<img src=\"./confusion_matrix.png\" alt=\"Eine beispielhafte Confusion-Matrix\" title=\"Eine beispielhafte Confusion Matrix\" />\n",
    "\n",
    "Die Zeilen dieser Matrix (bzw. Tabelle) entsprechen der tatsächlichen Klasse der Datenpunkte, die Spalten entsprechen der Klasse, die ein neuronales Netzwerk den einzelnen Datenpunkten zugeordnet hat. D.h. auf der diagonalen dieser Matrix stehen die Datenpunkte, bei denen die tatsächliche Klasse der zugeordneten Klasse entspricht.\n",
    "\n",
    "In diesem Beispiel wurden 850 ($8.5\\cdot10^2$ in Kurzschreibweise) Datenpunkte, die die wahre Klasse \"normal\" haben, von Netzwerk auch der Klasse \"normal\" zugeordnet. Ebenso wurden 530 ($5.3\\cdot10^2$ in Kurzschreibweise) Datenpunkte, die die wahre Klasse \"pneumonia\" haben, auch der Klasse \"pneumonia\" zugeordnet. Das Netzwerk hat aber auch 78 Datenpunkte, die die wahre Klasse \"pneumonia\" haben, der Klasse \"normal\" zugeordnet.\n",
    "\n",
    "#### Ein paar kleine Aufgaben:\n",
    "\n",
    "- Wie viele Bilder, die von COVID-Fällen stammen, wurden tatsächlich der Klasse \"covid\" zugeordnet?\n",
    "- Wie viele Bilder, die von COVID-Fällen stammen, wurden fälschlicher Weise der Klasse \"normal\" zugeordnet?\n",
    "- Was ist die Klasse, die das Netzwerk in diesem Beispiel einem Bild, das von einem COVID-Fall stammt, am __wahrscheinlichsten__ zuordnet?\n",
    "\n",
    "Überlegen sie und diskutieren sie mit ihren Mitstudierenden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Klicken sie hier für die Lösungen</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "Wie viele Bilder, die von COVID-Fällen stammen, wurden tatsächlich der Klasse \"covid\" zugeordnet?\n",
    "    \n",
    "__9__    \n",
    "\n",
    "Wie viele Bilder, die von COVID-Fällen stammen, wurden fälschlicher Weise der Klasse \"normal\" zugeordnet?\n",
    "\n",
    "__17__\n",
    "\n",
    "Was ist die Klasse, die das Netzwerk in diesem Beispiel einem Bild, das von einem COVID-Fall stammt, am wahrscheinlichsten zuordnet?\n",
    "    \n",
    "Die Klasse _\"pneumonia\"_. Diese wurde nämlich $36$ von insgesamt $(17+36+9)$ Fällen zugeordnet. Das entspricht $36/(17+36+9)\\approx 0.58 = 58\\%$ der COVID-Bilder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision und Recall aus der Confusion Matrix bestimmen\n",
    "\n",
    "Aus der Confusion-Matrix kann man tatsächlich auch Precision und Recall für die einzelnen Klassen bestimmen. Das geht so:\n",
    "\n",
    "#### Precision\n",
    "Erinnern sie sich, dass die Precision für eine Klasse K als die Wahrscheinlichkeit definiert ist, mit der ein Datenpunkt tatsächlich zur Klasse K gehört, wenn das Netz ihm die Klasse K zuordnet. Aus der oben dargestellten Confusion-Matrix können wir mit dieser Definition zum Beispiel die Precision für die Klasse \"normal\" bestimmen. Dazu bestimmen wir die Anzahl der Datenpunkte, die tatsächlich zur Klasse \"normal\" gehören und denen die Klasse \"normal\" zugeordnet wurde. Das sind in diesem Beispiel 850 Stück (Zeile \"normal\" und Spalte \"normal\"). Diese Anzahl teilen wir durch alle Datenpunkte, die vom Netzwerk der Klasse \"normal\" zugeordnet wurden. Das sind $850+78+17$ (Spaltensumme \"normal\"). Damit erhalten wir für die Precision im Hinblick auf die Klasse \"normal\" (für __dieses Netzwerk__ und __diesen Datensatz__):\n",
    "$$\\mathrm{ Precision(\"normal\") = P(Datenpunkt\\ hat\\ tatsächlich\\ Klasse\\ \"normal\"\\ |\\ Classifier\\ ordnet\\ Klasse\\ \"normal\"\\ zu) } \\\\\n",
    "= \\frac{850}{850+78+17} \\approx 0.90 = 90\\%$$\n",
    "\n",
    "#### Recall\n",
    "Um den Recall für eine Klasse K zu berechnen, bestimmen wir die Wahrscheinlichkeit, dass der Klassifier einem Datenpunkt die Klasse K zuordnet, wenn dieser Tatenpunkt tatsächlich die Klasse K hat. Aus der oben dargestellten Confusion-Matrix können wir diese Größe, z.B. für die Klasse \"normal\", wie folgt bestimmen: Wir bestimmen zunächst die Anzahl aller Datenpunkte, die tatsächlich zur Klasse \"normal\" gehören und denen die Klasse \"normal\" zugeordnet wurde. Das sind in diesem Beispiel 850 Stück (Zeile \"normal\" und Spalte \"normal\"). Dann teilen wir diese Anzahl durch die Anzahl aller Datenpunkte, die tatsächlich zur Klasse \"normal\" gehören. Dies sind in unserem Beispiel $850+35+0$ (Zeilensumme \"normal\"). Damit erhalten wir für den Recall im Hinblick auf die Klasse \"normal\" (für __dieses Netzwerk__):\n",
    "$$\\mathrm{ Recall(\"normal\") = P(Classifier\\ ordnet\\ Klasse\\ \"normal\"\\ zu\\ |\\ Datenpunkt\\ hat\\ tatsächlich\\ Klasse\\ \"normal\") } \\\\\n",
    "= \\frac{850}{850+35+0} \\approx 0.96 = 96\\%$$\n",
    "\n",
    "#### Noch zwei letzte Aufgaben:\n",
    "\n",
    "- Bestimmen sie für dieses Beispiel die Precision und den Recall im Hinblick auf die Klasse \"covid\".\n",
    "- Diskutieren sie mit ihren Mitstudierenden, was dies im Hinblick auf die Leistung des Netzwerks zur Detektion von COVID bedeutet, wenn sie die Begriffe \"Precision\" und \"Recall\" ähnlich wie die bekannten Begriffe \"Positiver Prädiktiver Wert\" und \"Sensitivität\" betrachten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Klicken sie hier für die Lösungen</summary>\n",
    "<p>\n",
    "\n",
    "- Bestimmen sie für dieses Beispiel die Precision und den Recall im Hinblick auf die Klasse \"covid\".\n",
    "\n",
    "##### Precision:    \n",
    "Wir bestimmen zuerst die Anzahl der Datenpunkte, die tatsächlich zur Klasse \"covid\" gehören und denen die Klasse \"covid\" zugeordnet wurde. Das sind in diesem Beispiel 9 Stück (Zeile \"covid\" und Spalte \"covid\"). Diese Anzahl teilen wir durch alle Datenpunkte, die vom Netzwerk der Klasse \"covid\" zugeordnet wurden. Das sind $0+0+9$ (Spaltensumme \"covid\"). Damit erhalten wir für die Precision im Hinblick auf die Klasse \"covid\" (für __dieses Netzwerk__ und __diesen Datensatz__):\n",
    "$$\\mathrm{ Precision(\"covid\") = P(Datenpunkt\\ hat\\ tatsächlich\\ Klasse\\ \"covid\"\\ |\\ Classifier\\ ordnet\\ Klasse\\ \"covid\"\\ zu) } \\\\\n",
    "= \\frac{9}{0+0+9} \\approx 1.0 = 100\\%$$\n",
    "    \n",
    "#### Recall:\n",
    "Wir bestimmen zunächst die Anzahl aller Datenpunkte, die tatsächlich zur Klasse \"covid\" gehören und denen die Klasse \"covid\" zugeordnet wurde. Das sind in diesem Beispiel 9 Stück (Zeile \"covid\" und Spalte \"covid\"). Dann teilen wir diese Anzahl durch die Anzahl aller Datenpunkte, die tatsächlich zur Klasse \"covid\" gehören. Dies sind in unserem Beispiel $17+36+9$ (Zeilensumme \"covid\"). Damit erhalten wir für den Recall im Hinblick auf die Klasse \"covid\" (für __dieses Netzwerk__):\n",
    "$$\\mathrm{ Recall(\"covid\") = P(Classifier\\ ordnet\\ Klasse\\ \"covid\"\\ zu\\ |\\ Datenpunkt\\ hat\\ tatsächlich\\ Klasse\\ \"covid\") } \\\\\n",
    "= \\frac{9}{17+36+9} \\approx 0.15 = 15\\%$$\n",
    "    \n",
    "- Diskutieren sie mit ihren Mitstudierenden, was dies im Hinblick auf die Leistung des Netzwerks zur Detektion von COVID bedeutet, wenn sie die Begriffe \"Precision\" und \"Recall\" ähnlich wie die bekannten Begriffe \"Positiver Prädiktiver Wert\" und \"Sensitivität\" betrachten.\n",
    "    \n",
    "Eine Precision von 1.0 bedeutet, dass in diesem Beispiel eine Zuordnung zur Klasse \"covid\" durch das Netzwerk bedeutet, dass der Datenpunkt praktisch sicher auch wirklich zur Klasse \"covid\" gehört (ähnlich einem Positiven Prädiktiven Wert von 100%).\n",
    "    \n",
    "Ein Recall von 0.15 bedeutet jedoch auch, dass von allen Datenpunkten, die wirklich zur Klasse \"covid\" gehören, nur 15% von dem Netzwerk entdeckt werden (ähnlich einer Sensitivität von 15%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jetzt geht es los: Training\n",
    "In der nächsten Zelle trainieren wir das Netz. Das dauert in der Regel sehr lange, bei modernen, riesigen Datensätzen zum Teil Tage bis Wochen. Unser Modell müsste wahrscheinlich auch ca. ein bis zwei Tage trainieren, bis es seine finale Performance erreicht. Jedoch hoffen wir, dass wir in den 1-2 Stunden, die wir heute zur Verfügung haben, schon erste Erfolge sehen.\n",
    "\n",
    "Bis jetzt war der Code, den wir Ihnen präsentiert haben sehr stark unterteilt und gekapselt. Bei der Trainingsschleife handelt es sich jedoch um einem sehr zentralen Punkt im Code, wenn man neuronale Netze trainiert. \n",
    "\n",
    "Im Prinzip passiert folgendes: \n",
    "\n",
    "- Die Trainingsschleife iteriert mehrere male über alle Daten im Trainingsset. Ein Durchlauf, in dem jeder Trainingsdatenpunkt dem Netz einmal präsentiert wird, wird als \"Epoche\" bezeichnet. D.h. ein Netzwerk, das für 50 Epochen trainiert wurde, hat jeden Trainingsdatenpunkt 50 mal gesehen.\n",
    "\n",
    "    * In jedem dieser Durchläufe wird das Trainingsset zufällig in einzelne Trainingsbatches geteilt. Diese werden dem Netzwerk nacheinander präsentiert, um damit den Gradienten (i.e. die Ableitung) der Zielfunktion nach den Netzwerkparametern zu bestimmen.\n",
    "\n",
    "    * Diese Gradienten werden vom Optimizer benutzt, um die Netzwerkparameter einen kleinen \"Schritt\" in eine Richtung zu bewegen, so dass die Zielfunktion kleiner wird.\n",
    "\n",
    "- Nach jeder Epoche, werden entsprechende Performancemetriken, nämlich der Mittelwert der Zielfunktion, die Accuracy und der Recall für die einzelnen Klassen, sowohl für das Trainingsset, als auch für das Validierungsset bestimmt.\n",
    "\n",
    "    * Diese Metriken werden in einer Log-Datei gespeichert, damit man sich später den Verlauf dieser Kennwerte anschauen kann, um die Konvergenz des Trainings beurteilen zu können. Z.B. sollte die Zielfunktion auf dem Trainingsset immer kleiner und die Accuracy auf dem Trainingsset immer größer werden, wenn man die Lernrate nicht zu groß gewählt hat.\n",
    "\n",
    "    * Entsprechend dieser Metriken bestimmt der \"Early-Stopper\" ob das Training angehalten werden sollte, oder ob die Lernrate eventuell verkleinert werden sollte.\n",
    "\n",
    "Im Folgenden präsentieren wir Ihnen hier die gesamte Trainingsschleife, jedoch mit sehr ausführlichen Kommentaren im Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 641,
     "referenced_widgets": [
      "3e58cfc94b6b4646a2b80ef340932bbf",
      "433ac10f9658440aa5819db572c14aad",
      "66ea2d7f069c414e97fc01a0c5106b88",
      "b6b21c7a8af1434abe72abb274a95ba9",
      "7912329b94d749beb0924e66496fdf3b",
      "c9670b4c64004f2bbb340c4242a3ab7c",
      "c95ba5eea3ba4f11a5bbfe4c7d16d033",
      "e3c517bd6a09446f96c71992ecec3d87"
     ]
    },
    "id": "OTticHaOMv2A",
    "outputId": "dc23168f-928f-4e60-9ba3-7cb786bcfd05",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tqdm hat einen geheimnisvollen Namen, ist aber nur dafür da, dass man ein bisschen Feedback bekommt,\n",
    "# wie schnell eine Schleife durchläuft, ohne viel an seinem Code ändern zu müssen. \n",
    "# Man muss nur \"for ... in ...\" durch for \"for ... in tqdm(...)\" ersetzen.\n",
    "from tqdm.notebook import tqdm\n",
    "from helper_functions_covid import compute_metrics, log_metrics_to_file\n",
    "\n",
    "best_model = model\n",
    "best_val_score = 0\n",
    "\n",
    "### In dieser Textdatei speichern wir unsere \"Trainingskurve\".\n",
    "### Diese zeichnet auf, wie sich die Kostenfunktion auf dem Trainings- und dem\n",
    "### Validierungsset, sowie einige andere Kennwerte während des Trainings entwickeln\n",
    "log_file = 'log_train.txt'\n",
    "\n",
    "n_epochs = 100  # Wenn man dieses Skript (bis morgen) durchlaufen lassen würde, \n",
    "                # würde es 100 mal durch das gesamten Trainingsset gehen, d.h. es hätte dann jedes Bild\n",
    "                # im Trainingsset 100 mal gesehen.\n",
    "\n",
    "# Wir zählen \"epoch\" von 0 bis n_epochs - 1 hoch.\n",
    "# In jedem Durchlauf dieser Schleife gehen wir einmal durch\n",
    "# das gesamte Trainingsset\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Wir setzen das Modell in den Trainingsmodus\n",
    "    model.train()    \n",
    "    \n",
    "    # In dieser Variablen werden wir die Kostenfunktion für diesen Durchlauf durch\n",
    "    # das gesamte Trainingsset akkumulieren.\n",
    "    train_loss = 0\n",
    "    \n",
    "    # In dieser Variablen werden wir zählen, wie viele der Trainingsbeispiele \n",
    "    # wir bei diesem Durchlauf durch das Trainingsset richtig Klassifiziert haben\n",
    "    train_correct = 0\n",
    "    \n",
    "    # Hiermit gehen wir einmal durch das gesamte Trainingsset\n",
    "    # Der \"train_loader\" gibt uns in jeder Iteration\n",
    "    # ein zufällig gezogenes Batch (\"data\") von Bildern aus dem Trainingsset.\n",
    "    # Ausserdem sagt er uns, bei der wievielten Iteration wir gerade sind (iter_num).\n",
    "    # \"train_loader\" achtet dabei selbstständig darauf, dass wir jedes\n",
    "    # Bild des Trainingssets genau einmal sehen.\n",
    "    #\n",
    "    # tqdm zeigt uns beim Durchlaufen an, wie lange eine Iteration dauert \n",
    "    # und bei welcher Iteration wir gerade sind. \n",
    "    for iter_num, data in tqdm(enumerate(train_loader)):\n",
    "        \n",
    "        # Lade für ein Trainingsbatch die entsprechenden Bilder (image)\n",
    "        # und die wahren Klassenlabel (target)\n",
    "        image, target = data['img'].to(device), data['label'].to(device)     \n",
    "\n",
    "        # Berechne den Output unseres Netzwerkes für das aktuelle \n",
    "        # Trainingsbatch\n",
    "        output = model(image)\n",
    "        \n",
    "        # Berechne die Kostenfunktion auf Grundlage des Netzwerk-Outputs\n",
    "        # und der wahren Klassen der Trainingsbeispiele (target)\n",
    "        #\n",
    "        # Wir teilen das loss durch 8, da wir noch einen kleinen Trick verwenden (s.u.), um das\n",
    "        # Training stabiler zu machen: Wir summieren erst die Gradienten von 8 Trainings-\n",
    "        # Batches auf, bevor wir diese an den Optimizer übergeben, der dann basierend darauf\n",
    "        # einen Optimierungsschritt macht. Das entspricht (wenn wir das Loss hier entsprechend teilen)\n",
    "        # genau der Dynamik, die wir bekommen würden, wenn wir unsere Batch-Size verachtfachen würden.\n",
    "        # Allerdings brauchen wir so nicht 8-mal soviel Arbeitsspeicher auf der Graphikkarte.\n",
    "        loss = criterion(output, target.long()) / 8\n",
    "        \n",
    "        # Wir summieren die Kostenfunktion für die einzelnen Trainingsbatches\n",
    "        # auf, um am Schluss den Mittelwert über das gesamte Trainingsset zu berechnen.\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # HIER, genau HIER wird der Gradient der Zielfunktion für die einzelnen\n",
    "        # Parameter des Netzes per BACKPROPAGATION ausgerechnet. Magic.\n",
    "        loss.backward()\n",
    "\n",
    "        # Jeder Aufruf von \"backward\" berechnet die entsprechenden Gradienten und speichert diese, \n",
    "        # so lange, bis der Optimizer diese einmal benutzt um einen Schritt zu machen\n",
    "        \n",
    "        # Hier benutzen wir einen subtilen Trick, um etwas das Rauschen im Optimierungsprozess zu reduzieren:\n",
    "        # Wir machen nur jeder 8. Iteration einen Schritt des Optimizers. D.h. wir Akkumulieren zunächst die\n",
    "        # Gradienten für 8 Trainings-Batches. Dann nutzen wir diese *auf einmal* um einen Schritt zu gehen.\n",
    "        # Im Prinzip entspricht das der gleichen Trainingsdynamik, als würden wir die Größe\n",
    "        # unserer Trainings-Batches *verachtfachen*. Nur, dass wir 64 Traininsbeispiele *auf einmal*\n",
    "        # nicht in unserem Graphikkartenspeicher unterbekommen würden.\n",
    "        \n",
    "        # iter_num % 8 ist gleich Null genau dann, wenn iter_num ein vielfaches von 8 ist.\n",
    "        # Also für iter_num = 0, 8, 16, 24, 32, 40, 48, ...\n",
    "        if iter_num % 8 == 0:\n",
    "            \n",
    "            # D.h. jede 8te Iteration machen wir einen Schritt mit unserem Optimizer\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Nach dem Schritt setzen wir die (mit der loss.backward()-Funktion)\n",
    "            # akkumulierten Gradienten wieder auf 0 zurück.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "        # Wir berechnen die Vorhersage unseres Netzes für die Trainingsbeispiele,\n",
    "        # in dem wir für jedes Trainingsbeispiel die Klasse mit der maximalen Wahrscheinlichkeit\n",
    "        # bestimmen. Das ist die Klasse, deren Output-Neuron (output) am aktivsten ist. \n",
    "        # Dies wird über die \"argmax\"-Funktion bestimmt.\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        # Jetzt bestimmen wir die Anzahl der Trainingsbeispiele, bei denen die vorhergesagte\n",
    "        # Klasse der wahren Klasse entspricht, also die Anzahl der korrekt klassifizierten\n",
    "        # Trainingsbeispiele\n",
    "        correct = pred.eq(target.long().view_as(pred)).sum().item()\n",
    "        \n",
    "        # Da wir auch hiervon später den Mittelwert über das gesamte Trainingsset bilden\n",
    "        # wollen, summieren wir dies auch auf.\n",
    "        train_correct += correct\n",
    "        \n",
    "    # Am Ende jeder Epoche, d.h. wenn das Training einmal über alle Datenpunkte im Trainingsset gelaufen ist,\n",
    "    # berechnen wir ein paar Kennwerte auf dem Trainings- und auf dem VALIDIERUNGSSET.\n",
    "    \n",
    "    # Wir berechnen zunächst die Accuracy, d.h. den relativen Anteil der richtig klassifizierten \n",
    "    # Datenpunkte, auf dem Trainingsset.\n",
    "    # Dafür teilen wir die insgesamt richtig klassifizierten Datenpunkte durch\n",
    "    # die Anzahl der Datenpunkte im Trainingsset:\n",
    "    train_accuracy = train_correct/len(train_loader.dataset)\n",
    "    \n",
    "    # Ausserdem berechnen wir den Mittelwert der Zielfunktion (Cross-Entropy)\n",
    "    # auf dem Trainingsset\n",
    "    mean_train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    # Wir geben die Metriken auf dem Trainingsset aus:\n",
    "    print('\\nTraining Performance Epoch %d: Average loss: %f, Accuracy: %d/%d (%f Percent)\\n' % \\\n",
    "          (epoch, mean_train_loss, train_correct, len(train_loader.dataset), 100.0 * train_accuracy) )\n",
    "     \n",
    "    # Nun berechnen wir ein paar Kennwerte auf dem VALIDIERUNGSSET. Beachte: wir übergeben der Funktion den\n",
    "    # Daten-Loader \"val_loader\" des Validierungssets.\n",
    "    metrics_dict = compute_metrics(model, val_loader, device)\n",
    "    \n",
    "    # Wir geben die entsprechenden Statistiken (schön formatiert) aus:\n",
    "    print('------------------ Epoch {} ---------------------------------------------------'.format(epoch))\n",
    "    print(\"Accuracy \\t {:.3f}\".format(metrics_dict['accuracy']))\n",
    "    print(\"Recall on normal validation data \\t {:.3f}\".format(metrics_dict['recall_normal']))\n",
    "    print(\"Recall on pneumonia validation data \\t {:.3f}\".format(metrics_dict['recall_pneumonia']))\n",
    "    print(\"Recall on COVID validation data \\t {:.3f}\".format(metrics_dict['recall_covid']))\n",
    "    print(\"Val Loss \\t {}\".format(metrics_dict[\"validation loss\"]))\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Wir speichern das Modell mit der besten Accuracy auf dem\n",
    "    # ***VALIDIERUNGSSET***\n",
    "    if metrics_dict['accuracy'] > best_val_score:\n",
    "        torch.save(model, \"best_model.pkl\")\n",
    "        best_val_score = metrics_dict['accuracy']\n",
    "    \n",
    "    # Wir speichern die Metriken in der log_file\n",
    "    save_metrics = [epoch, mean_train_loss, metrics_dict[\"validation loss\"], train_accuracy, metrics_dict[\"accuracy\"], \\\n",
    "                    metrics_dict[\"recall_normal\"], metrics_dict[\"recall_pneumonia\"], metrics_dict[\"recall_covid\"]]\n",
    "    log_metrics_to_file(log_file, save_metrics, epoch)\n",
    "    \n",
    "    # Wir informieren auch unseren \"Early-Stopper\" über den Trainings-Fortschritt auf dem\n",
    "    # Validierungsset\n",
    "    early_stopper.add_data(model, metrics_dict['validation loss'], metrics_dict['accuracy'])\n",
    "    \n",
    "    # Wenn sowohl die Zielfunktion als auch die Accuracy auf dem Validierungsset über einen \n",
    "    # längeren Zeitraum (5 Epochen) kontiniuerlich ansteigen, beenden wir den Trainingsvorgang\n",
    "    # vorzeitig. Genau das ist mit \"early stopping\" gemeint.\n",
    "    if early_stopper.stop() == 1:\n",
    "        break\n",
    "    \n",
    "    # Wenn die Accuracy noch zunimmt, das Loss jedoch nicht mehr, versuchen wir zunächst, die\n",
    "    # Lernrate zu verkleinern, in dem wir sie mit einem Faktor 0.1 multiplizieren.\n",
    "    if early_stopper.stop() == 3:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            learning_rate *= 0.1\n",
    "            param_group['lr'] = learning_rate\n",
    "            print('Updating the learning rate to {}'.format(learning_rate))\n",
    "            early_stopper.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Herzlichen Glückwunsch!__ Wenn die Trainingsschleife läuft, haben sie sich eine kurze Pause verdient, bevor wir mit den Referaten weitermachen. Während sie ihren Mitstudierenden lauschen, kann ihr Netzwerk nun einige Epochen der Trainingsdaten sehen, bevor wir uns die Trainingskurven und die Performance des Netzwerks auf dem Testset später mit dem zweiten Notebook in diesem Ordner genauer anschauen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train_Classifier_on_COVID-Chest-X-Rays.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
