{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Workshop_Landing_Pad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop: Transformer-based Large Language Models\n",
        "\n",
        "Today we want to explore **Transformer based Large Language Models (LLMs)**. These are a class of deep-learning models, which recently took the throne in learning structured representations of text and solving many complex tasks, such as question answering, translation, text summarization, etc... Although [ChatGPT](https://openai.com/blog/chatgpt) by OpenAI is one of the most widely discussed language models right now, one major driver of innovation in language modeling was the Transformer architecture introduced in a [publication](https://arxiv.org/pdf/1706.03762.pdf) from Google Brain. This paper still trained the model on a single, specific task, namely machine translation. The next major breakthrough was the realization that by task-agnostic pre-training, using so called \"self-supervised learning\" or \"generative modeling\" tasks, the model already can learn much of the relevant statistical structure of written text, which it then can leverage to quickly be adapted to multiple downstream tasks (by so-called \"fine-tuning\" of a pre-trained \"foundation model\"). One of the first models to display the ability to generate long, coherent texts based on this approach were Google's [BERT](https://arxiv.org/abs/1810.04805v2), and OpenAI's [GPT-2](https://openai.com/research/better-language-models) in 2019. The ability of such language models to generalize to more and more tasks by scaling both the model size, in terms of the trainable parameters, and the training datasets was impressively demonstrated by Google's [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) in 2022, as nicely illustrated in this figure from the accompagnying [blog post](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html):\n",
        "\n",
        "<img src = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLXCWMlipdu0gFF6hsiJHbxg1zSaEkdDWfl-8RakQuW__8RPvlOS9KGIScNCytxT4jz9isnx0GLMwbS1G0Q4WdXzT42GszgfwIIAVX1H3J-43lVWWqcb--q9cPsxCsJFFz2dRfpKgEmLe-xfIyBqQuPq1BPYcK9CtAK1_xnhgvgAAx0GeZmODJxGNMYQ/s16000/image8.gif\">"
      ],
      "metadata": {
        "id": "eljScTnDPhSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model already displayed some quite sophisticated language understanding, e.g., in its ability to explain jokes: \n",
        "\n",
        "<img src = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKfA6Vxw9li1vDfDChv-yzCp4ubKpOR2D62IMui3mAe5Un0jZ3P2_60stEZdtJNUt1b2XNzXbPoM0EO6B7UGneMd-2Bq0JK0gC5rIMSgQM02jwe5VmGxYLo_jz78vnG79oDIpv3mNu6kD0tqAUT6pcYkbkRpeoO9P-92I5O8ZsZefCpcxfIfEJREAyA/s1999/image5.png\">\n",
        "\n",
        "It also shows some (at least statistical) \"understanding\" of cause and effect, which it displayed in counterfactual (i.e., answering \"what-if\" questions) reasoning tasks:\n",
        "\n",
        "<img src = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLmQjS3gOQ2x7ru3xovYjVw-Yr2fKDCqhDHByQZitD92Yu4L-v2BBa5f_VMfpWM4D0930Dmk35EY1TqGrYUtMQqJO41hkLqXuu51eOpXZ3PvYPSjf5stfEJNJn2idWnRYCCEgBiJuLDTXX5Fgt-Mk13kCKdO12JShGvDO_cArtLKv8U8obJaHiL5ASQg/s1320/Big%20Bench%20Sped%20Up%20Cropped.gif\">\n",
        "\n",
        "Besides the model architecture and training data, also the inputs to the model, the so called \"prompts\" were optimized. E.g., by asking the model to not directly jump to a conclusion, but instead create a chain of reasoning, the quality of its answers in common-sense reasoning tasks could be improved substantially: \n",
        "\n",
        "<img src = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCZkpChUnyojDE_nPmg-xZyTFgjo4qgBB7Pmbi0ZlIVNiiD2DXV1dcMW-QMIn-CTNNTA7bJlln0p8wuNju06E62adtn4C-sRngwKhvhA1-f0-8knYuWB-m3MyIXclYAQNkojWaf-kfibm1OjFfhC45EHkeJkVNKid-K2dd_O1c5H-rVx8ypOTuQv8ELQ/s1228/Screenshot%202022-04-01%205.34.54%20PM.png\">\n",
        "\n",
        "But not only (multiple) natural, but also artificial (programming) languages could be learned within a single model, which lead to the observation that generative language modeling can also be helpful in computer programming tasks:\n",
        "\n",
        "<img src = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6gdPPoicGB0nweCccX9zN8mEwp8Nye503s604BUI1EO36-iTJNe3cJptm1lpWX5oEvXRoQl8cCRtYI1UkD5bfkN5zModWSoxMOGIlxBbNvI-g4SMsdEWZTDbR-ifbV7HQK0VX0yWXsSqGbR_vu_gtAUJ5BgJyAODkxiqA6I2GKkFn3mrKzKbd0aZVzA/s16000/Coding%20Examples%20v2%20Cropped.gif\">\n",
        "\n",
        "**Feel free to test this later and ask some large language model to help you with some of our exercises, for example the impressive model by [Perplexity AI](https://www.perplexity.ai/)**"
      ],
      "metadata": {
        "id": "MERvwQMdKHZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For everyone, who hasn't used Python or Google Colab before, we've prepared some introductory notebooks:\n",
        "\n",
        "*   [Introduction to Python and Colab](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Jupyter_Intro/colab_intro.ipynb). This is strongly suggested, in case you have no programming experience or this is your first time using Colab. If you're running your own Jupyter server at home, feel free to skip this.\n",
        "*   [Calculating and plotting PPVs](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Jupyter_Intro/ppv_exercise.ipynb). This notebook teaches you, how to use Python to calculate and display the positive predictive value of a test with given sensitivity and specificity as a function of the pre-test probability. Here you can try to fill some code-cells yourself to get acquainted with Python and Colab.\n",
        "*   [Program a little multiple choice quiz](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Jupyter_Intro/mc_exercise.ipynb). In this notebook, you can implement a short script, which interactively reads some multiple-choice questions from a textfile, asks them to a user and automatically checks the answers for correctness. Here you can get some more familiarity with the logical control structures, such as loops and if-clauses.\n",
        "\n",
        "Once you are sufficiently comfortable with using an interactive Python environment, we suggest you to start your journey into transformer-based language modeling with these notebooks:\n",
        "\n",
        "*   [Overview over Huggingface Transformers Pipelines](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Transformers/01_Transformer_Intro.ipynb). This is the notebook gives you a first overview, of how to use the great [Huggingface ðŸ¤—](https://huggingface.co/) \"transformers\" library to load and use pre-trained language models. Huggingface collects a whole zoo of pre-trained deep-learning models across almost all architectures and applications (image processing, video, language, text, and many more), feel free to take a couple of minutes (or longer) to have a look at their demo [\"spaces\"](https://huggingface.co/spaces), which allow you to interact with many models directly on their homepage. Huggingface does not only provide you with the models, which can easily be used in your Python code, but also with very informative model cards for each trained network. In these cards you can read more about the individual architecture, how it was trained, on which dataset it was trained, and how its performance was validated. But the model card should also include sections on the model's limitations, algorithmic bias, environmental footprint, potential misuse and malicious use, and other relevant issues, which must be considered when training or using large machine learning ressources. You will hear more on this in the later notebooks, but here are the model cards of a [German question-answering model](https://huggingface.co/Sahajtomar/GBERTQnA), and a ChatGPT-style [conversational large-language model](https://huggingface.co/decapoda-research/llama-7b-hf), which we will be using and exploring today.\n",
        "\n",
        "After you've got your first impression of how to use pre-trained transformer models within Colab notebooks, we'll dive a bit more into detail with two notebooks, which will teach you how to combine a pre-trained language model with some preprocessing to answer questions based on Wikipedia articles:\n",
        "\n",
        "*   [Built your own question-answering pipeline from individual components](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Transformers/02_Question_Answering_with_BERT.ipynb)\n",
        "* [Use your own question-answering pipeline to answer questions based on Wikipedia articles](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Transformers/03_Question_Answering_using_Wikipedia.ipynb)\n",
        "\n",
        "Finally, we will show you, how powerful modern, condensed language models already are, which can be run on affordable consumer hardware (or on free Google Colab instances):\n",
        "\n",
        "*   [Interact with a conversational large-language model running on a single, free Google Colab instance](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Transformers/04_Open_Source_Conversation_Model.ipynb): This demo downloads a 7-Billion-Parameter large language model to a free Google Colab instance and creates an interactive interface, so that you can interact with this model in a fashion similar to ChatGPT or Perplexity AI.\n",
        "*  [Build your own code around a general-purpose large language model](https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Transformers/05_Open_Source_Conversation_Model_Playground.ipynb): This notebook loads the same model as the previous one, but instead of supplying you with a graphical user interface to interact with it in a ChatGPT-like manner, the model interface is encapsulated in a small function. This allows you to directly input text to the model and get the model's predicted output, so that you can use this functionality flexibly in your own Python projects.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xK-5l06KVJ0F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srhLhvsgYREx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}